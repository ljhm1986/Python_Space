{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "FastText Model\n",
    "==============\n",
    "\n",
    "Introduces Gensim's fastText model and demonstrates its use on the Lee Corpus.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we'll learn to work with fastText library for training word-embedding\n",
    "models, saving & loading them and performing similarity operations & vector\n",
    "lookups analogous to Word2Vec.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When to use FastText?\n",
    "---------------------\n",
    "\n",
    "The main principle behind `fastText <https://github.com/facebookresearch/fastText>`_ is that the morphological structure of a word carries important information about the meaning of the word, which is not taken into account by traditional word embeddings, which train a unique word embedding for every individual word. This is especially significant for morphologically rich languages (German, Turkish) in which a single word can have a large number of morphological forms, each of which might occur rarely, thus making it hard to train good word embeddings.\n",
    "\n",
    "\n",
    "fastText attempts to solve this by treating each word as the aggregation of its subwords. For the sake of simplicity and language-independence, subwords are taken to be the character ngrams of the word. The vector for a word is simply taken to be the sum of all vectors of its component char-ngrams.\n",
    "\n",
    "\n",
    "According to a detailed comparison of Word2Vec and FastText in `this notebook <https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/Word2Vec_FastText_Comparison.ipynb>`__, fastText does significantly better on syntactic tasks as compared to the original Word2Vec, especially when the size of the training corpus is small. Word2Vec slightly outperforms FastText on semantic tasks though. The differences grow smaller as the size of training corpus increases.\n",
    "\n",
    "\n",
    "Training time for fastText is significantly higher than the Gensim version of Word2Vec (\\ ``15min 42s`` vs ``6min 42s`` on text8, 17 mil tokens, 5 epochs, and a vector size of 100).\n",
    "\n",
    "\n",
    "fastText can be used to obtain vectors for out-of-vocabulary (OOV) words, by summing up vectors for its component char-ngrams, provided at least one of the char-ngrams was present in the training data.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training models\n",
    "---------------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the following examples, we'll use the Lee Corpus (which you already have if you've installed gensim) for training our model.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pprint import pprint as print\n",
    "from gensim.models.fasttext import FastText as FT_gensim\n",
    "from gensim.test.utils import datapath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-10 21:48:52,594 : INFO : resetting layer weights\n"
     ]
    }
   ],
   "source": [
    "# Set file names for train and test data\n",
    "corpus_file = datapath('lee_background.cor')\n",
    "\n",
    "model = FT_gensim(size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-10 21:49:00,819 : INFO : collecting all words and their counts\n",
      "2020-03-10 21:49:00,820 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2020-03-10 21:49:00,835 : INFO : collected 10781 word types from a corpus of 59890 raw words and 300 sentences\n",
      "2020-03-10 21:49:00,835 : INFO : Loading a fresh vocabulary\n",
      "2020-03-10 21:49:01,003 : INFO : effective_min_count=5 retains 1762 unique words (16% of original 10781, drops 9019)\n",
      "2020-03-10 21:49:01,004 : INFO : effective_min_count=5 leaves 46084 word corpus (76% of original 59890, drops 13806)\n",
      "2020-03-10 21:49:01,009 : INFO : deleting the raw counts dictionary of 10781 items\n",
      "2020-03-10 21:49:01,010 : INFO : sample=0.001 downsamples 45 most-common words\n",
      "2020-03-10 21:49:01,010 : INFO : downsampling leaves estimated 32610 word corpus (70.8% of prior 46084)\n",
      "2020-03-10 21:49:01,026 : INFO : estimated required memory for 1762 words, 16936 buckets and 100 dimensions: 9411496 bytes\n",
      "2020-03-10 21:49:01,027 : INFO : resetting layer weights\n"
     ]
    }
   ],
   "source": [
    "# build the vocabulary\n",
    "model.build_vocab(corpus_file=corpus_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-10 21:49:10,372 : INFO : training model with 3 workers on 1762 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2020-03-10 21:49:10,463 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-10 21:49:10,465 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-10 21:49:10,467 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-10 21:49:10,467 : INFO : EPOCH - 1 : training on 60387 raw words (32958 effective words) took 0.1s, 363784 effective words/s\n",
      "2020-03-10 21:49:10,566 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-10 21:49:10,568 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-10 21:49:10,578 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-10 21:49:10,579 : INFO : EPOCH - 2 : training on 60387 raw words (32906 effective words) took 0.1s, 307599 effective words/s\n",
      "2020-03-10 21:49:10,669 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-10 21:49:10,699 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-10 21:49:10,701 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-10 21:49:10,701 : INFO : EPOCH - 3 : training on 60387 raw words (32863 effective words) took 0.1s, 277540 effective words/s\n",
      "2020-03-10 21:49:10,792 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-10 21:49:10,806 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-10 21:49:10,810 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-10 21:49:10,811 : INFO : EPOCH - 4 : training on 60387 raw words (32832 effective words) took 0.1s, 311694 effective words/s\n",
      "2020-03-10 21:49:10,904 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-10 21:49:10,929 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-10 21:49:10,933 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-10 21:49:10,934 : INFO : EPOCH - 5 : training on 60387 raw words (32827 effective words) took 0.1s, 278200 effective words/s\n",
      "2020-03-10 21:49:10,934 : INFO : training on a 301935 raw words (164386 effective words) took 0.6s, 292718 effective words/s\n",
      "2020-03-10 21:49:10,935 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "model.train(\n",
    "    corpus_file=corpus_file, epochs=model.epochs,\n",
    "    total_examples=model.corpus_count, total_words=model.corpus_total_words\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<gensim.models.fasttext.FastText object at 0x00000173FBC41A90>\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training hyperparameters\n",
    "^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters for training the model follow the same pattern as Word2Vec. FastText supports the following parameters from the original word2vec:\n",
    "\n",
    "- model: Training architecture. Allowed values: `cbow`, `skipgram` (Default `cbow`)\n",
    "- size: Size of embeddings to be learnt (Default 100)\n",
    "- alpha: Initial learning rate (Default 0.025)\n",
    "- window: Context window size (Default 5)\n",
    "- min_count: Ignore words with number of occurrences below this (Default 5)\n",
    "- loss: Training objective. Allowed values: `ns`, `hs`, `softmax` (Default `ns`)\n",
    "- sample: Threshold for downsampling higher-frequency words (Default 0.001)\n",
    "- negative: Number of negative words to sample, for `ns` (Default 5)\n",
    "- iter: Number of epochs (Default 5)\n",
    "- sorted_vocab: Sort vocab by descending frequency (Default 1)\n",
    "- threads: Number of threads to use (Default 12)\n",
    "\n",
    "\n",
    "In addition, FastText has three additional parameters:\n",
    "\n",
    "- min_n: min length of char ngrams (Default 3)\n",
    "- max_n: max length of char ngrams (Default 6)\n",
    "- bucket: number of buckets used for hashing ngrams (Default 2000000)\n",
    "\n",
    "\n",
    "Parameters ``min_n`` and ``max_n`` control the lengths of character ngrams that each word is broken down into while training and looking up embeddings. If ``max_n`` is set to 0, or to be lesser than ``min_n``\\ , no character ngrams are used, and the model effectively reduces to Word2Vec.\n",
    "\n",
    "\n",
    "\n",
    "To bound the memory requirements of the model being trained, a hashing function is used that maps ngrams to integers in 1 to K. For hashing these character sequences, the `Fowler-Noll-Vo hashing function <http://www.isthe.com/chongo/tech/comp/fnv>`_ (FNV-1a variant) is employed.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** As in the case of Word2Vec, you can continue to train your model while using Gensim's native implementation of fastText.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving/loading models\n",
    "---------------------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models can be saved and loaded via the ``load`` and ``save`` methods.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-10 21:41:41,207 : INFO : saving FastText object under C:\\Users\\user\\AppData\\Local\\Temp\\saved_model_gensim-7lhybykg, separately []\n",
      "2020-03-10 21:41:41,208 : INFO : storing np array 'vectors_ngrams' to C:\\Users\\user\\AppData\\Local\\Temp\\saved_model_gensim-7lhybykg.wv.vectors_ngrams.npy\n",
      "2020-03-10 21:41:43,147 : INFO : not storing attribute vectors_norm\n",
      "2020-03-10 21:41:43,148 : INFO : not storing attribute vectors_vocab_norm\n",
      "2020-03-10 21:41:43,149 : INFO : not storing attribute vectors_ngrams_norm\n",
      "2020-03-10 21:41:43,149 : INFO : not storing attribute buckets_word\n",
      "2020-03-10 21:41:43,150 : INFO : storing np array 'vectors_ngrams_lockf' to C:\\Users\\user\\AppData\\Local\\Temp\\saved_model_gensim-7lhybykg.trainables.vectors_ngrams_lockf.npy\n",
      "2020-03-10 21:41:49,713 : INFO : saved C:\\Users\\user\\AppData\\Local\\Temp\\saved_model_gensim-7lhybykg\n",
      "2020-03-10 21:41:49,714 : INFO : loading FastText object from C:\\Users\\user\\AppData\\Local\\Temp\\saved_model_gensim-7lhybykg\n",
      "2020-03-10 21:41:49,742 : INFO : loading wv recursively from C:\\Users\\user\\AppData\\Local\\Temp\\saved_model_gensim-7lhybykg.wv.* with mmap=None\n",
      "2020-03-10 21:41:49,743 : INFO : loading vectors_ngrams from C:\\Users\\user\\AppData\\Local\\Temp\\saved_model_gensim-7lhybykg.wv.vectors_ngrams.npy with mmap=None\n",
      "2020-03-10 21:41:50,102 : INFO : setting ignored attribute vectors_norm to None\n",
      "2020-03-10 21:41:50,103 : INFO : setting ignored attribute vectors_vocab_norm to None\n",
      "2020-03-10 21:41:50,104 : INFO : setting ignored attribute vectors_ngrams_norm to None\n",
      "2020-03-10 21:41:50,104 : INFO : setting ignored attribute buckets_word to None\n",
      "2020-03-10 21:41:50,104 : INFO : loading vocabulary recursively from C:\\Users\\user\\AppData\\Local\\Temp\\saved_model_gensim-7lhybykg.vocabulary.* with mmap=None\n",
      "2020-03-10 21:41:50,105 : INFO : loading trainables recursively from C:\\Users\\user\\AppData\\Local\\Temp\\saved_model_gensim-7lhybykg.trainables.* with mmap=None\n",
      "2020-03-10 21:41:50,105 : INFO : loading vectors_ngrams_lockf from C:\\Users\\user\\AppData\\Local\\Temp\\saved_model_gensim-7lhybykg.trainables.vectors_ngrams_lockf.npy with mmap=None\n",
      "2020-03-10 21:41:50,470 : INFO : loaded C:\\Users\\user\\AppData\\Local\\Temp\\saved_model_gensim-7lhybykg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<gensim.models.fasttext.FastText object at 0x0000017428879710>\n"
     ]
    }
   ],
   "source": [
    "# saving a model trained via Gensim's fastText implementation\n",
    "import tempfile\n",
    "import os\n",
    "with tempfile.NamedTemporaryFile(prefix='saved_model_gensim-', delete=False) as tmp:\n",
    "    model.save(tmp.name, separately=[])\n",
    "\n",
    "loaded_model = FT_gensim.load(tmp.name)\n",
    "print(loaded_model)\n",
    "\n",
    "os.unlink(tmp.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ``save_word2vec_method`` causes the vectors for ngrams to be lost. As a result, a model loaded in this way will behave as a regular word2vec model.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word vector lookup\n",
    "------------------\n",
    "\n",
    "\n",
    "**Note:** Operations like word vector lookups and similarity queries can be performed in exactly the same manner for both the implementations of fastText so they have been demonstrated using only the native fastText implementation here.\n",
    "\n",
    "\n",
    "\n",
    "FastText models support vector lookups for out-of-vocabulary words by summing up character ngrams belonging to the word.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print('night' in model.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print('nights' in model.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([ 9.49415863e-02,  5.90373576e-03, -5.72208643e-01,  4.88739759e-01,\n",
      "        5.98190427e-01, -3.24555606e-01, -1.84535354e-01, -3.69616263e-02,\n",
      "        4.30120021e-01,  3.38728935e-01, -6.35988057e-01, -1.13690756e-02,\n",
      "       -6.64123476e-01,  4.12354976e-01,  2.88462251e-01, -6.14066683e-02,\n",
      "       -1.87915862e-01,  1.82341829e-01,  2.39225477e-01, -3.75126839e-01,\n",
      "       -2.33614624e-01,  2.77896821e-01, -3.78508836e-01,  1.73061173e-02,\n",
      "       -8.30545664e-01,  7.31233835e-01,  1.13920026e-01,  1.73350975e-01,\n",
      "        4.07674164e-01,  3.57048376e-03, -6.53613985e-01,  2.31604159e-01,\n",
      "        8.53485018e-02, -4.74979430e-01,  4.63648021e-01,  1.14711665e-01,\n",
      "       -1.62452549e-01, -7.59977624e-02,  4.28185731e-01,  2.23893389e-01,\n",
      "       -8.57773237e-03, -7.43460655e-02,  3.91313285e-01, -6.72177300e-02,\n",
      "        1.26174882e-01,  1.87195867e-01, -1.37181431e-01,  2.17835397e-01,\n",
      "       -2.08233222e-02, -3.85169327e-01, -5.67647398e-01, -5.56722224e-01,\n",
      "        7.44956955e-02,  1.11433696e-02,  4.11040097e-01, -7.99529016e-01,\n",
      "       -1.12100586e-01, -1.73298433e-01,  1.42438747e-02, -3.19594368e-02,\n",
      "        2.09253788e-01, -1.35670394e-01, -4.83152062e-01, -9.21977088e-02,\n",
      "       -5.33723891e-01,  3.63094896e-01, -1.92181215e-05,  1.58050284e-01,\n",
      "        3.30429412e-02,  4.74011332e-01, -5.63884437e-01, -5.07336199e-01,\n",
      "       -1.52222617e-02, -1.01442099e-01, -3.44504178e-01,  3.27991955e-02,\n",
      "        2.74999619e-01,  1.04654863e-01, -1.41858637e-01,  1.82055488e-01,\n",
      "        4.47084725e-01,  6.41888008e-02, -1.75675377e-01,  4.50614840e-01,\n",
      "       -4.00832295e-01, -2.62129486e-01,  1.29885167e-01,  2.92728573e-01,\n",
      "        3.15638423e-01,  2.97870278e-01, -1.64584935e-01,  1.13683507e-01,\n",
      "       -3.75952907e-02, -1.92538634e-01, -2.68526375e-01,  5.49119234e-01,\n",
      "        2.81346351e-01,  4.33241963e-01,  5.54004133e-01,  4.72859055e-01],\n",
      "      dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\user\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "print(model['night'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([ 8.34019780e-02,  5.68733318e-03, -4.97448713e-01,  4.24134493e-01,\n",
      "        5.19104481e-01, -2.83573598e-01, -1.61089286e-01, -3.13409492e-02,\n",
      "        3.73554826e-01,  2.95112610e-01, -5.54936111e-01, -1.13063566e-02,\n",
      "       -5.77557564e-01,  3.59845072e-01,  2.46771231e-01, -5.30997775e-02,\n",
      "       -1.65473521e-01,  1.56806290e-01,  2.06319243e-01, -3.26517493e-01,\n",
      "       -2.03059167e-01,  2.42494136e-01, -3.29739124e-01,  1.57074369e-02,\n",
      "       -7.22689390e-01,  6.35452747e-01,  9.89861786e-02,  1.51157930e-01,\n",
      "        3.54554296e-01,  2.68665841e-03, -5.66109478e-01,  2.02572212e-01,\n",
      "        7.29037821e-02, -4.15093541e-01,  4.02934372e-01,  9.94668230e-02,\n",
      "       -1.39799282e-01, -6.73641115e-02,  3.72268140e-01,  1.95475519e-01,\n",
      "       -6.56093145e-03, -6.47969022e-02,  3.39752942e-01, -5.67338914e-02,\n",
      "        1.08628184e-01,  1.63001820e-01, -1.22839451e-01,  1.91182479e-01,\n",
      "       -1.87458098e-02, -3.33474368e-01, -4.96106029e-01, -4.83274043e-01,\n",
      "        6.59496412e-02,  1.06344791e-02,  3.58028263e-01, -6.95339680e-01,\n",
      "       -9.75740626e-02, -1.49558961e-01,  1.00482833e-02, -2.74870638e-02,\n",
      "        1.81533888e-01, -1.18820325e-01, -4.19921935e-01, -8.07766393e-02,\n",
      "       -4.64589924e-01,  3.15148026e-01,  9.76193842e-05,  1.37313202e-01,\n",
      "        3.04911397e-02,  4.12945300e-01, -4.90321159e-01, -4.40721571e-01,\n",
      "       -1.32638402e-02, -8.90898705e-02, -2.99324393e-01,  2.77231243e-02,\n",
      "        2.39209861e-01,  9.12034288e-02, -1.21419631e-01,  1.57506570e-01,\n",
      "        3.89558017e-01,  5.56498170e-02, -1.53609768e-01,  3.90968025e-01,\n",
      "       -3.47565502e-01, -2.27362797e-01,  1.15005985e-01,  2.55885154e-01,\n",
      "        2.73908406e-01,  2.59820133e-01, -1.43499613e-01,  9.86629874e-02,\n",
      "       -3.40841971e-02, -1.67212248e-01, -2.33382642e-01,  4.77933466e-01,\n",
      "        2.43775979e-01,  3.75406116e-01,  4.81492460e-01,  4.11320210e-01],\n",
      "      dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\user\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "print(model['nights'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ``in`` operation works slightly differently from the original word2vec. It tests whether a vector for the given word exists or not, not whether the word is present in the word vocabulary. To test whether a word is present in the training word vocabulary -\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tests if word present in vocab\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(\"word\" in model.wv.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tests if vector present for word\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\user\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "print(\"word\" in model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarity operations\n",
    "---------------------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarity operations work the same way as word2vec. **Out-of-vocabulary words can also be used, provided they have at least one character ngram present in the training data.**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(\"nights\" in model.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(\"night\" in model.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9999927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\user\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "print(model.similarity(\"night\", \"nights\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Syntactically similar words generally have high similarity in fastText models, since a large number of the component char-ngrams will be the same. As a result, fastText generally does better at syntactic tasks than Word2Vec. A detailed comparison is provided `here <Word2Vec_FastText_Comparison.ipynb>`_.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other similarity operations\n",
    "^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "\n",
    "The example training corpus is a toy corpus, results are not expected to be good, for proof-of-concept only\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\user\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "2020-03-10 21:42:33,048 : INFO : precomputing L2-norms of word weight vectors\n",
      "2020-03-10 21:42:33,049 : INFO : precomputing L2-norms of ngram weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('study', 0.9982730150222778),\n",
      " ('boat', 0.9982665777206421),\n",
      " ('often', 0.9982637166976929),\n",
      " ('Arafat', 0.9982549548149109),\n",
      " ('north.', 0.9982534646987915),\n",
      " ('\"That', 0.9982510209083557),\n",
      " ('Arafat,', 0.998248815536499),\n",
      " ('Endeavour', 0.9982373118400574),\n",
      " ('stage', 0.9982361197471619),\n",
      " ('beyond', 0.9982330799102783)]\n"
     ]
    }
   ],
   "source": [
    "print(model.most_similar(\"nights\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.99995035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\user\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `n_similarity` (Method will be removed in 4.0.0, use self.wv.n_similarity() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "print(model.n_similarity(['sushi', 'shop'], ['japanese', 'restaurant']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'breakfast'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\user\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `doesnt_match` (Method will be removed in 4.0.0, use self.wv.doesnt_match() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "c:\\users\\user\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\gensim\\models\\keyedvectors.py:877: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  vectors = vstack(self.word_vec(word, use_norm=True) for word in used_words).astype(REAL)\n"
     ]
    }
   ],
   "source": [
    "print(model.doesnt_match(\"breakfast cereal dinner lunch\".split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('1', 0.2431073784828186),\n",
      " ('40', 0.2382356971502304),\n",
      " ('2', 0.23509719967842102),\n",
      " ('26', 0.23366421461105347),\n",
      " ('20', 0.23335930705070496),\n",
      " ('blaze', 0.23280811309814453),\n",
      " ('UN', 0.23270395398139954),\n",
      " ('keep', 0.2319527119398117),\n",
      " ('...', 0.23165921866893768),\n",
      " ('As', 0.23148386180400848)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\user\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "print(model.most_similar(positive=['baghdad', 'england'], negative=['london']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\user\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `accuracy` (Method will be removed in 4.0.0, use self.wv.accuracy() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "2020-03-10 21:42:52,953 : INFO : family: 0.0% (0/2)\n",
      "2020-03-10 21:42:52,966 : INFO : gram3-comparative: 25.0% (3/12)\n",
      "2020-03-10 21:42:52,974 : INFO : gram4-superlative: 8.3% (1/12)\n",
      "2020-03-10 21:42:52,982 : INFO : gram5-present-participle: 20.0% (4/20)\n",
      "2020-03-10 21:42:52,992 : INFO : gram6-nationality-adjective: 35.0% (7/20)\n",
      "2020-03-10 21:42:53,001 : INFO : gram7-past-tense: 5.0% (1/20)\n",
      "2020-03-10 21:42:53,009 : INFO : gram8-plural: 8.3% (1/12)\n",
      "2020-03-10 21:42:53,012 : INFO : total: 17.3% (17/98)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'correct': [], 'incorrect': [], 'section': 'capital-common-countries'},\n",
      " {'correct': [], 'incorrect': [], 'section': 'capital-world'},\n",
      " {'correct': [], 'incorrect': [], 'section': 'currency'},\n",
      " {'correct': [], 'incorrect': [], 'section': 'city-in-state'},\n",
      " {'correct': [],\n",
      "  'incorrect': [('HE', 'SHE', 'HIS', 'HER'), ('HIS', 'HER', 'HE', 'SHE')],\n",
      "  'section': 'family'},\n",
      " {'correct': [], 'incorrect': [], 'section': 'gram1-adjective-to-adverb'},\n",
      " {'correct': [], 'incorrect': [], 'section': 'gram2-opposite'},\n",
      " {'correct': [('GOOD', 'BETTER', 'GREAT', 'GREATER'),\n",
      "              ('GREAT', 'GREATER', 'LOW', 'LOWER'),\n",
      "              ('LONG', 'LONGER', 'GREAT', 'GREATER')],\n",
      "  'incorrect': [('GOOD', 'BETTER', 'LONG', 'LONGER'),\n",
      "                ('GOOD', 'BETTER', 'LOW', 'LOWER'),\n",
      "                ('GREAT', 'GREATER', 'LONG', 'LONGER'),\n",
      "                ('GREAT', 'GREATER', 'GOOD', 'BETTER'),\n",
      "                ('LONG', 'LONGER', 'LOW', 'LOWER'),\n",
      "                ('LONG', 'LONGER', 'GOOD', 'BETTER'),\n",
      "                ('LOW', 'LOWER', 'GOOD', 'BETTER'),\n",
      "                ('LOW', 'LOWER', 'GREAT', 'GREATER'),\n",
      "                ('LOW', 'LOWER', 'LONG', 'LONGER')],\n",
      "  'section': 'gram3-comparative'},\n",
      " {'correct': [('GREAT', 'GREATEST', 'LARGE', 'LARGEST')],\n",
      "  'incorrect': [('BIG', 'BIGGEST', 'GOOD', 'BEST'),\n",
      "                ('BIG', 'BIGGEST', 'GREAT', 'GREATEST'),\n",
      "                ('BIG', 'BIGGEST', 'LARGE', 'LARGEST'),\n",
      "                ('GOOD', 'BEST', 'GREAT', 'GREATEST'),\n",
      "                ('GOOD', 'BEST', 'LARGE', 'LARGEST'),\n",
      "                ('GOOD', 'BEST', 'BIG', 'BIGGEST'),\n",
      "                ('GREAT', 'GREATEST', 'BIG', 'BIGGEST'),\n",
      "                ('GREAT', 'GREATEST', 'GOOD', 'BEST'),\n",
      "                ('LARGE', 'LARGEST', 'BIG', 'BIGGEST'),\n",
      "                ('LARGE', 'LARGEST', 'GOOD', 'BEST'),\n",
      "                ('LARGE', 'LARGEST', 'GREAT', 'GREATEST')],\n",
      "  'section': 'gram4-superlative'},\n",
      " {'correct': [('PLAY', 'PLAYING', 'SAY', 'SAYING'),\n",
      "              ('PLAY', 'PLAYING', 'LOOK', 'LOOKING'),\n",
      "              ('SAY', 'SAYING', 'LOOK', 'LOOKING'),\n",
      "              ('SAY', 'SAYING', 'PLAY', 'PLAYING')],\n",
      "  'incorrect': [('GO', 'GOING', 'LOOK', 'LOOKING'),\n",
      "                ('GO', 'GOING', 'PLAY', 'PLAYING'),\n",
      "                ('GO', 'GOING', 'RUN', 'RUNNING'),\n",
      "                ('GO', 'GOING', 'SAY', 'SAYING'),\n",
      "                ('LOOK', 'LOOKING', 'PLAY', 'PLAYING'),\n",
      "                ('LOOK', 'LOOKING', 'RUN', 'RUNNING'),\n",
      "                ('LOOK', 'LOOKING', 'SAY', 'SAYING'),\n",
      "                ('LOOK', 'LOOKING', 'GO', 'GOING'),\n",
      "                ('PLAY', 'PLAYING', 'RUN', 'RUNNING'),\n",
      "                ('PLAY', 'PLAYING', 'GO', 'GOING'),\n",
      "                ('RUN', 'RUNNING', 'SAY', 'SAYING'),\n",
      "                ('RUN', 'RUNNING', 'GO', 'GOING'),\n",
      "                ('RUN', 'RUNNING', 'LOOK', 'LOOKING'),\n",
      "                ('RUN', 'RUNNING', 'PLAY', 'PLAYING'),\n",
      "                ('SAY', 'SAYING', 'GO', 'GOING'),\n",
      "                ('SAY', 'SAYING', 'RUN', 'RUNNING')],\n",
      "  'section': 'gram5-present-participle'},\n",
      " {'correct': [('AUSTRALIA', 'AUSTRALIAN', 'INDIA', 'INDIAN'),\n",
      "              ('AUSTRALIA', 'AUSTRALIAN', 'ISRAEL', 'ISRAELI'),\n",
      "              ('FRANCE', 'FRENCH', 'INDIA', 'INDIAN'),\n",
      "              ('FRANCE', 'FRENCH', 'ISRAEL', 'ISRAELI'),\n",
      "              ('INDIA', 'INDIAN', 'ISRAEL', 'ISRAELI'),\n",
      "              ('INDIA', 'INDIAN', 'AUSTRALIA', 'AUSTRALIAN'),\n",
      "              ('ISRAEL', 'ISRAELI', 'INDIA', 'INDIAN')],\n",
      "  'incorrect': [('AUSTRALIA', 'AUSTRALIAN', 'FRANCE', 'FRENCH'),\n",
      "                ('AUSTRALIA', 'AUSTRALIAN', 'SWITZERLAND', 'SWISS'),\n",
      "                ('FRANCE', 'FRENCH', 'SWITZERLAND', 'SWISS'),\n",
      "                ('FRANCE', 'FRENCH', 'AUSTRALIA', 'AUSTRALIAN'),\n",
      "                ('INDIA', 'INDIAN', 'SWITZERLAND', 'SWISS'),\n",
      "                ('INDIA', 'INDIAN', 'FRANCE', 'FRENCH'),\n",
      "                ('ISRAEL', 'ISRAELI', 'SWITZERLAND', 'SWISS'),\n",
      "                ('ISRAEL', 'ISRAELI', 'AUSTRALIA', 'AUSTRALIAN'),\n",
      "                ('ISRAEL', 'ISRAELI', 'FRANCE', 'FRENCH'),\n",
      "                ('SWITZERLAND', 'SWISS', 'AUSTRALIA', 'AUSTRALIAN'),\n",
      "                ('SWITZERLAND', 'SWISS', 'FRANCE', 'FRENCH'),\n",
      "                ('SWITZERLAND', 'SWISS', 'INDIA', 'INDIAN'),\n",
      "                ('SWITZERLAND', 'SWISS', 'ISRAEL', 'ISRAELI')],\n",
      "  'section': 'gram6-nationality-adjective'},\n",
      " {'correct': [('PAYING', 'PAID', 'SAYING', 'SAID')],\n",
      "  'incorrect': [('GOING', 'WENT', 'PAYING', 'PAID'),\n",
      "                ('GOING', 'WENT', 'PLAYING', 'PLAYED'),\n",
      "                ('GOING', 'WENT', 'SAYING', 'SAID'),\n",
      "                ('GOING', 'WENT', 'TAKING', 'TOOK'),\n",
      "                ('PAYING', 'PAID', 'PLAYING', 'PLAYED'),\n",
      "                ('PAYING', 'PAID', 'TAKING', 'TOOK'),\n",
      "                ('PAYING', 'PAID', 'GOING', 'WENT'),\n",
      "                ('PLAYING', 'PLAYED', 'SAYING', 'SAID'),\n",
      "                ('PLAYING', 'PLAYED', 'TAKING', 'TOOK'),\n",
      "                ('PLAYING', 'PLAYED', 'GOING', 'WENT'),\n",
      "                ('PLAYING', 'PLAYED', 'PAYING', 'PAID'),\n",
      "                ('SAYING', 'SAID', 'TAKING', 'TOOK'),\n",
      "                ('SAYING', 'SAID', 'GOING', 'WENT'),\n",
      "                ('SAYING', 'SAID', 'PAYING', 'PAID'),\n",
      "                ('SAYING', 'SAID', 'PLAYING', 'PLAYED'),\n",
      "                ('TAKING', 'TOOK', 'GOING', 'WENT'),\n",
      "                ('TAKING', 'TOOK', 'PAYING', 'PAID'),\n",
      "                ('TAKING', 'TOOK', 'PLAYING', 'PLAYED'),\n",
      "                ('TAKING', 'TOOK', 'SAYING', 'SAID')],\n",
      "  'section': 'gram7-past-tense'},\n",
      " {'correct': [('MAN', 'MEN', 'CHILD', 'CHILDREN')],\n",
      "  'incorrect': [('BUILDING', 'BUILDINGS', 'CAR', 'CARS'),\n",
      "                ('BUILDING', 'BUILDINGS', 'CHILD', 'CHILDREN'),\n",
      "                ('BUILDING', 'BUILDINGS', 'MAN', 'MEN'),\n",
      "                ('CAR', 'CARS', 'CHILD', 'CHILDREN'),\n",
      "                ('CAR', 'CARS', 'MAN', 'MEN'),\n",
      "                ('CAR', 'CARS', 'BUILDING', 'BUILDINGS'),\n",
      "                ('CHILD', 'CHILDREN', 'MAN', 'MEN'),\n",
      "                ('CHILD', 'CHILDREN', 'BUILDING', 'BUILDINGS'),\n",
      "                ('CHILD', 'CHILDREN', 'CAR', 'CARS'),\n",
      "                ('MAN', 'MEN', 'BUILDING', 'BUILDINGS'),\n",
      "                ('MAN', 'MEN', 'CAR', 'CARS')],\n",
      "  'section': 'gram8-plural'},\n",
      " {'correct': [], 'incorrect': [], 'section': 'gram9-plural-verbs'},\n",
      " {'correct': [('GOOD', 'BETTER', 'GREAT', 'GREATER'),\n",
      "              ('GREAT', 'GREATER', 'LOW', 'LOWER'),\n",
      "              ('LONG', 'LONGER', 'GREAT', 'GREATER'),\n",
      "              ('GREAT', 'GREATEST', 'LARGE', 'LARGEST'),\n",
      "              ('PLAY', 'PLAYING', 'SAY', 'SAYING'),\n",
      "              ('PLAY', 'PLAYING', 'LOOK', 'LOOKING'),\n",
      "              ('SAY', 'SAYING', 'LOOK', 'LOOKING'),\n",
      "              ('SAY', 'SAYING', 'PLAY', 'PLAYING'),\n",
      "              ('AUSTRALIA', 'AUSTRALIAN', 'INDIA', 'INDIAN'),\n",
      "              ('AUSTRALIA', 'AUSTRALIAN', 'ISRAEL', 'ISRAELI'),\n",
      "              ('FRANCE', 'FRENCH', 'INDIA', 'INDIAN'),\n",
      "              ('FRANCE', 'FRENCH', 'ISRAEL', 'ISRAELI'),\n",
      "              ('INDIA', 'INDIAN', 'ISRAEL', 'ISRAELI'),\n",
      "              ('INDIA', 'INDIAN', 'AUSTRALIA', 'AUSTRALIAN'),\n",
      "              ('ISRAEL', 'ISRAELI', 'INDIA', 'INDIAN'),\n",
      "              ('PAYING', 'PAID', 'SAYING', 'SAID'),\n",
      "              ('MAN', 'MEN', 'CHILD', 'CHILDREN')],\n",
      "  'incorrect': [('HE', 'SHE', 'HIS', 'HER'),\n",
      "                ('HIS', 'HER', 'HE', 'SHE'),\n",
      "                ('GOOD', 'BETTER', 'LONG', 'LONGER'),\n",
      "                ('GOOD', 'BETTER', 'LOW', 'LOWER'),\n",
      "                ('GREAT', 'GREATER', 'LONG', 'LONGER'),\n",
      "                ('GREAT', 'GREATER', 'GOOD', 'BETTER'),\n",
      "                ('LONG', 'LONGER', 'LOW', 'LOWER'),\n",
      "                ('LONG', 'LONGER', 'GOOD', 'BETTER'),\n",
      "                ('LOW', 'LOWER', 'GOOD', 'BETTER'),\n",
      "                ('LOW', 'LOWER', 'GREAT', 'GREATER'),\n",
      "                ('LOW', 'LOWER', 'LONG', 'LONGER'),\n",
      "                ('BIG', 'BIGGEST', 'GOOD', 'BEST'),\n",
      "                ('BIG', 'BIGGEST', 'GREAT', 'GREATEST'),\n",
      "                ('BIG', 'BIGGEST', 'LARGE', 'LARGEST'),\n",
      "                ('GOOD', 'BEST', 'GREAT', 'GREATEST'),\n",
      "                ('GOOD', 'BEST', 'LARGE', 'LARGEST'),\n",
      "                ('GOOD', 'BEST', 'BIG', 'BIGGEST'),\n",
      "                ('GREAT', 'GREATEST', 'BIG', 'BIGGEST'),\n",
      "                ('GREAT', 'GREATEST', 'GOOD', 'BEST'),\n",
      "                ('LARGE', 'LARGEST', 'BIG', 'BIGGEST'),\n",
      "                ('LARGE', 'LARGEST', 'GOOD', 'BEST'),\n",
      "                ('LARGE', 'LARGEST', 'GREAT', 'GREATEST'),\n",
      "                ('GO', 'GOING', 'LOOK', 'LOOKING'),\n",
      "                ('GO', 'GOING', 'PLAY', 'PLAYING'),\n",
      "                ('GO', 'GOING', 'RUN', 'RUNNING'),\n",
      "                ('GO', 'GOING', 'SAY', 'SAYING'),\n",
      "                ('LOOK', 'LOOKING', 'PLAY', 'PLAYING'),\n",
      "                ('LOOK', 'LOOKING', 'RUN', 'RUNNING'),\n",
      "                ('LOOK', 'LOOKING', 'SAY', 'SAYING'),\n",
      "                ('LOOK', 'LOOKING', 'GO', 'GOING'),\n",
      "                ('PLAY', 'PLAYING', 'RUN', 'RUNNING'),\n",
      "                ('PLAY', 'PLAYING', 'GO', 'GOING'),\n",
      "                ('RUN', 'RUNNING', 'SAY', 'SAYING'),\n",
      "                ('RUN', 'RUNNING', 'GO', 'GOING'),\n",
      "                ('RUN', 'RUNNING', 'LOOK', 'LOOKING'),\n",
      "                ('RUN', 'RUNNING', 'PLAY', 'PLAYING'),\n",
      "                ('SAY', 'SAYING', 'GO', 'GOING'),\n",
      "                ('SAY', 'SAYING', 'RUN', 'RUNNING'),\n",
      "                ('AUSTRALIA', 'AUSTRALIAN', 'FRANCE', 'FRENCH'),\n",
      "                ('AUSTRALIA', 'AUSTRALIAN', 'SWITZERLAND', 'SWISS'),\n",
      "                ('FRANCE', 'FRENCH', 'SWITZERLAND', 'SWISS'),\n",
      "                ('FRANCE', 'FRENCH', 'AUSTRALIA', 'AUSTRALIAN'),\n",
      "                ('INDIA', 'INDIAN', 'SWITZERLAND', 'SWISS'),\n",
      "                ('INDIA', 'INDIAN', 'FRANCE', 'FRENCH'),\n",
      "                ('ISRAEL', 'ISRAELI', 'SWITZERLAND', 'SWISS'),\n",
      "                ('ISRAEL', 'ISRAELI', 'AUSTRALIA', 'AUSTRALIAN'),\n",
      "                ('ISRAEL', 'ISRAELI', 'FRANCE', 'FRENCH'),\n",
      "                ('SWITZERLAND', 'SWISS', 'AUSTRALIA', 'AUSTRALIAN'),\n",
      "                ('SWITZERLAND', 'SWISS', 'FRANCE', 'FRENCH'),\n",
      "                ('SWITZERLAND', 'SWISS', 'INDIA', 'INDIAN'),\n",
      "                ('SWITZERLAND', 'SWISS', 'ISRAEL', 'ISRAELI'),\n",
      "                ('GOING', 'WENT', 'PAYING', 'PAID'),\n",
      "                ('GOING', 'WENT', 'PLAYING', 'PLAYED'),\n",
      "                ('GOING', 'WENT', 'SAYING', 'SAID'),\n",
      "                ('GOING', 'WENT', 'TAKING', 'TOOK'),\n",
      "                ('PAYING', 'PAID', 'PLAYING', 'PLAYED'),\n",
      "                ('PAYING', 'PAID', 'TAKING', 'TOOK'),\n",
      "                ('PAYING', 'PAID', 'GOING', 'WENT'),\n",
      "                ('PLAYING', 'PLAYED', 'SAYING', 'SAID'),\n",
      "                ('PLAYING', 'PLAYED', 'TAKING', 'TOOK'),\n",
      "                ('PLAYING', 'PLAYED', 'GOING', 'WENT'),\n",
      "                ('PLAYING', 'PLAYED', 'PAYING', 'PAID'),\n",
      "                ('SAYING', 'SAID', 'TAKING', 'TOOK'),\n",
      "                ('SAYING', 'SAID', 'GOING', 'WENT'),\n",
      "                ('SAYING', 'SAID', 'PAYING', 'PAID'),\n",
      "                ('SAYING', 'SAID', 'PLAYING', 'PLAYED'),\n",
      "                ('TAKING', 'TOOK', 'GOING', 'WENT'),\n",
      "                ('TAKING', 'TOOK', 'PAYING', 'PAID'),\n",
      "                ('TAKING', 'TOOK', 'PLAYING', 'PLAYED'),\n",
      "                ('TAKING', 'TOOK', 'SAYING', 'SAID'),\n",
      "                ('BUILDING', 'BUILDINGS', 'CAR', 'CARS'),\n",
      "                ('BUILDING', 'BUILDINGS', 'CHILD', 'CHILDREN'),\n",
      "                ('BUILDING', 'BUILDINGS', 'MAN', 'MEN'),\n",
      "                ('CAR', 'CARS', 'CHILD', 'CHILDREN'),\n",
      "                ('CAR', 'CARS', 'MAN', 'MEN'),\n",
      "                ('CAR', 'CARS', 'BUILDING', 'BUILDINGS'),\n",
      "                ('CHILD', 'CHILDREN', 'MAN', 'MEN'),\n",
      "                ('CHILD', 'CHILDREN', 'BUILDING', 'BUILDINGS'),\n",
      "                ('CHILD', 'CHILDREN', 'CAR', 'CARS'),\n",
      "                ('MAN', 'MEN', 'BUILDING', 'BUILDINGS'),\n",
      "                ('MAN', 'MEN', 'CAR', 'CARS')],\n",
      "  'section': 'total'}]\n"
     ]
    }
   ],
   "source": [
    "print(model.accuracy(questions=datapath('questions-words.txt')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word Movers distance\n",
    "^^^^^^^^^^^^^^^^^^^^\n",
    "\n",
    "Let's start with two sentences:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_obama = 'Obama speaks to the media in Illinois'.lower().split()\n",
    "sentence_president = 'The president greets the press in Chicago'.lower().split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove their stopwords.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\user/nltk_data'\n    - 'c:\\\\users\\\\user\\\\appdata\\\\local\\\\programs\\\\python\\\\python36\\\\nltk_data'\n    - 'c:\\\\users\\\\user\\\\appdata\\\\local\\\\programs\\\\python\\\\python36\\\\share\\\\nltk_data'\n    - 'c:\\\\users\\\\user\\\\appdata\\\\local\\\\programs\\\\python\\\\python36\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\user\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mc:\\users\\user\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     85\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m                     \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'{}/{}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     87\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    700\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 701\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    702\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords.zip/stopwords/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\user/nltk_data'\n    - 'c:\\\\users\\\\user\\\\appdata\\\\local\\\\programs\\\\python\\\\python36\\\\nltk_data'\n    - 'c:\\\\users\\\\user\\\\appdata\\\\local\\\\programs\\\\python\\\\python36\\\\share\\\\nltk_data'\n    - 'c:\\\\users\\\\user\\\\appdata\\\\local\\\\programs\\\\python\\\\python36\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\user\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-b7b7132a0d8f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mstopwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'english'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0msentence_obama\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentence_obama\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0msentence_president\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentence_president\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    121\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    124\u001b[0m         \u001b[1;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m         \u001b[1;31m# __class__ to something new:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     86\u001b[0m                     \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'{}/{}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m         \u001b[1;31m# Load the corpus.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     81\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m                 \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'{}/{}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    699\u001b[0m     \u001b[0msep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'*'\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m70\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    700\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 701\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    702\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    703\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\user/nltk_data'\n    - 'c:\\\\users\\\\user\\\\appdata\\\\local\\\\programs\\\\python\\\\python36\\\\nltk_data'\n    - 'c:\\\\users\\\\user\\\\appdata\\\\local\\\\programs\\\\python\\\\python36\\\\share\\\\nltk_data'\n    - 'c:\\\\users\\\\user\\\\appdata\\\\local\\\\programs\\\\python\\\\python36\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\user\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('english')\n",
    "sentence_obama = [w for w in sentence_obama if w not in stopwords]\n",
    "sentence_president = [w for w in sentence_president if w not in stopwords]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute WMD.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\user\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `wmdistance` (Method will be removed in 4.0.0, use self.wv.wmdistance() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyemd'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-136d73d70ed4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdistance\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwmdistance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence_obama\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentence_president\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdistance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\gensim\\utils.py\u001b[0m in \u001b[0;36mnew_func1\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1445\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1446\u001b[0m                 )\n\u001b[1;32m-> 1447\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1448\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1449\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mnew_func1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\gensim\\models\\base_any2vec.py\u001b[0m in \u001b[0;36mwmdistance\u001b[1;34m(self, document1, document2)\u001b[0m\n\u001b[0;32m   1404\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1405\u001b[0m         \"\"\"\n\u001b[1;32m-> 1406\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwmdistance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocument1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdocument2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1407\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1408\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mdeprecated\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Method will be removed in 4.0.0, use self.wv.most_similar_cosmul() instead\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mwmdistance\u001b[1;34m(self, document1, document2)\u001b[0m\n\u001b[0;32m    715\u001b[0m         \u001b[1;31m# If pyemd C extension is available, import it.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    716\u001b[0m         \u001b[1;31m# If pyemd is attempted to be used, but isn't installed, ImportError will be raised in wmdistance\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 717\u001b[1;33m         \u001b[1;32mfrom\u001b[0m \u001b[0mpyemd\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0memd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    718\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    719\u001b[0m         \u001b[1;31m# Remove out-of-vocabulary words.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pyemd'"
     ]
    }
   ],
   "source": [
    "distance = model.wmdistance(sentence_obama, sentence_president)\n",
    "print(distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's all! You've made it to the end of this tutorial.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'fasttext-logo-color-web.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-b82b68130414>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimage\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmpimg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmpimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'fasttext-logo-color-web.png'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mimgplot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'off'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\matplotlib\\image.py\u001b[0m in \u001b[0;36mimread\u001b[1;34m(fname, format)\u001b[0m\n\u001b[0;32m   1372\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mhandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1373\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1374\u001b[1;33m             \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfd\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1375\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mhandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1376\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'fasttext-logo-color-web.png'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "img = mpimg.imread('fasttext-logo-color-web.png')\n",
    "imgplot = plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
